<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.8.1" />
<title>podomics.dataset API documentation</title>
<meta name="description" content="Dataset module for podomics." />
<link href='https://cdnjs.cloudflare.com/ajax/libs/normalize/8.0.0/normalize.min.css' rel='stylesheet'>
<link href='https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/8.0.0/sanitize.min.css' rel='stylesheet'>
<link href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css" rel="stylesheet">
<style>.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>podomics.dataset</code></h1>
</header>
<section id="section-intro">
<p>Dataset module for podomics.</p>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">&#34;&#34;&#34;Dataset module for podomics.&#34;&#34;&#34;

import numpy as np
import pandas
import matplotlib
from matplotlib import pyplot
from sklearn import preprocessing
from sklearn.preprocessing import MaxAbsScaler

def read_csv(file, sample=None, time=None, condition=None, **kwargs):
    &#34;&#34;&#34;
Import dataset from a CSV file.

TODO: info about file format.

Parameters
---
file : str, path, or buffer
    The csv file to read from.
sample : str, default=None
    Column header for sample labels. By default, no sample labels are used, and samples can only be indexed by integers.
time : str, default=None
    Column header for time stamps. 
    If not given, try to find a column header starting with &#34;Time&#34;; raise an error if not found.
condition : str, default=None
    Column header for conditions. By default, no conditions are considered.
**kwargs : 
    Are passed to [pandas.read_csv()](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_csv.html) and can be used to adapt to specifics in the file format, e.g.:
 * **`sep`**: separator string, pandas default is &#39;,&#39;
 * **`comment`**: character that identifies lines to be interpreted as comments

Returns
---
Dataset : object
    The `Dataset` object constructed from the data in the given file.

Example usage:
---
    &gt;&gt;&gt; omics = read_csv(&#34;examples/exampledata1.csv&#34;, sample=&#39;Sample&#39;)
    &#34;&#34;&#34;
    df = pandas.read_csv(file, index_col=sample, **kwargs)
    return Dataset(df, time, condition)

class Dataset(object):
    &#34;&#34;&#34;Data container and various data handling and plotting methods.
    &#34;&#34;&#34;
    def __init__(self, data, time=None, condition=None, features=None):
        &#34;&#34;&#34;The constructor creates a dataset from a pandas Dataframe.

Parameters
---
data : pandas DataFrame
    Contains the data to analyse.
    It should follow the column layout as explained under **`read_csv`**.
    If sample labels are used, they should be stored in the index of the DataFrame.
time : str, default=None
    Name of the column which contains the timestamps.
    If not given, try to find a column header starting with &#34;Time&#34;; raise an error if not found.
condition : str, default=None
    Name of the column which contains the conditions, no conditions are used if this is not given.
features : list of str, default=None
    Column headers to use as features. Use all remaining if not given.

Attributes
---
data : pandas DataFrame
    Access to the underlying DataFrame
time : str
    Column header used for timestamps
condition : str
    Column header used for condition labels
features : list of str
    Column headers that are referring to features in the dataset
condition_list : list of str
    List of unique condition labels found in the dataset
timepoints : array of numeric or str
    Sorted array of timestamps found in the dataset
scaling : object
    Class used for rescaling the data, see `Dataset.rescale`.

Example usage
---
    &gt;&gt;&gt; df = pandas.read_csv(&#34;examples/exampledata1.csv&#34;, index_col=&#34;Sample&#34;)
    &gt;&gt;&gt; omics = Dataset(df, time=&#34;Timepoint&#34;, features=[&#39;0&#39;, &#39;1&#39;, &#39;3&#39;, &#39;4&#39;, &#39;6&#39;])
    &gt;&gt;&gt; omics.time
    &#39;Timepoint&#39;
    &gt;&gt;&gt; omics.timepoints
    array([0.  , 0.25, 0.5 , 0.75, 1.  ])
        &#34;&#34;&#34;
        self.data = data
        
        num_cols = len(data.columns)
        if time in data.columns:
            self.time = time
        else:
            if time is None:
                # try to infer time column
                for c in data.columns:
                    if c.startswith(&#34;Time&#34;):
                        time = c
                if time is None:
                    raise ValueError(f&#34;Could not identify a time column in {data.columns[:min(num_cols, 5)]}..., use the `time` parameter to specify one.&#34;)
                else:
                    self.time = time
            else:
                raise ValueError(f&#39;Time column &#34;{time}&#34; not found in data columns: {data.columns[:min(num_cols, 5)]}...&#39;)
        # print(f&#34;Found time column: {self.time}&#34;)

        if condition is None:
            self.condition_list = []
        else:
            if condition in self.data.columns:
                self.condition_list = list(set(data[condition]))
            else:
                raise ValueError(f&#39;Condition column &#34;{condition}&#34; not found in data columns: {data.columns[:min(num_cols, 5)]}...&#39;)
        self.condition = condition
        # print(f&#34;Checked condition column: {self.condition}&#34;)
    
        if features is None:
            self.features = [f for f in data.columns if f not in (self.time, self.condition)]
        else:
            self.features = [f for f in features if (f in data.columns) and f not in (self.time, self.condition)]
            if len(self.features)==0:
                raise ValueError(f&#34;Did not find any of the given feature names in data columns: {self.data.columns[:min(num_cols, 5)]}...&#34;)
        # print(f&#34;Found {len(self.features)} features&#34;)
        # Check that all feature columns are numeric
        # https://stackoverflow.com/questions/19900202/how-to-determine-whether-a-column-variable-is-numeric-or-not-in-pandas-numpy
        from pandas.api.types import is_numeric_dtype
        for f in self.features:
            if not is_numeric_dtype(data[f]):
                raise ValueError(f&#39;Column &#34;{f}&#34; was interpreted as feature, but is not numeric:\n{self.data[f].head()}&#39;)
        # print(f&#34;Verified all feature columns contain numerical data.&#34;)

        self.scaling = None
        timepoints = self.data[self.time].unique()
        timepoints.sort()
        self.timepoints = timepoints

        if self.condition is None:
            columns = [self.time,] + self.features
        else:
            columns = [self.condition, self.time] + self.features
        self.data = data.loc[:, columns]
        # print(f&#34;Transfered data in dataframe of shape {self.data.shape}.&#34;)

        # sort rows first by condition if used, then by timepoint
        if self.condition is not None:
            self.data.sort_values(by=[self.condition, self.time], inplace=True)
        else:
            self.data.sort_values(by=[self.time], inplace=True)
        # print(f&#34;Sorted rows by conditions and timepoints.&#34;)


    def rescale(self, method=MaxAbsScaler(), conditions=None, features=None):
        &#34;&#34;&#34;Rescale the dataset with a transformation method from [sklearn.preprocessing](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.preprocessing).

The data can optionally be filtered with the parameters **`conditions`** and **`features`** before the scaling.

The scaled result is returned as a new **`Dataset`** object.
The applied scaling method is stored in the `scaling` attribute of the returned `Dataset` object.

Parameters
---
method : object, default=MaxAbsScaler() from sklearn.preprocessing
    Instance of a transformation class from [sklearn.preprocessing](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.preprocessing) to be used for resaling the data.
    For median scaling, the class `MedianScaler` provided in this module can be used.
conditions: list of str, default=None
    Restrict the data to conditions given here before scaling.
features: list of str, default=None
    Restrict the data to features given here before scaling.

Returns
---
Dataset : object
    Dataset with the scaled values.
    The `scaling` attribute is set to the object passed in the `method` parameter.

Example usage
---
#### Load a dataset

    &gt;&gt;&gt; omics = read_csv(&#34;examples/exampledata1.csv&#34;, sample=&#34;Sample&#34;)

#### Scaling with maximum absolute value
    &gt;&gt;&gt; scaled = omics.rescale()
    &gt;&gt;&gt; np.allclose([scaled.data[f].max() for f in scaled.features], 1.0)
    True

#### Scaling with median
    &gt;&gt;&gt; scaled = omics.rescale(method=MedianScaler())
    &gt;&gt;&gt; np.allclose([scaled.data[f].median() for f in scaled.features], 1.0)
    True
&#34;&#34;&#34;
        scaler = method
        if features is None:
            features = self.features
        if self.condition is None or conditions is None:
            df = self.data
        else:
            df = self.data[self.data[self.condition].isin(conditions)]
        method.fit(df.loc[:, features])
        df_scaled = df.copy()
        df_scaled.loc[:, features] = method.transform(df.loc[:, features])
        data_scaled = Dataset(df_scaled, time=self.time, condition=self.condition, features=features)
        data_scaled.scaling = method
        return data_scaled

    def plot(self, ax=None, condition=None, features=None, colormap=&#39;viridis&#39;, legend=True):
        &#34;&#34;&#34;Plot the dataset.

This method creates a scatterplot where feature values are plotted vs. feature index.
Data from different timepoints can be colored differently via a colormap.

Parameters
---
ax : matplotlib Axes, default=None
    If given: plot into these axes, otherwise create a new figure to plot in.
condition : str or list of str, default=None
    Choose the condition (from `Dataset.condition_list`) to plot.
    If a list is given, plot all conditions in the list in subplots. This is only possible with a newly created figure.
    Default is to plot all conditions in the dataset.
features : list of str, default=None
    List of features to include in the plot.
    Default is to plot all features in the dataset.
colormap : str or matplotlib Colormap, default=&#39;viridis&#39;
    Colormap to use for choosing the color for each timepoint.
legend : Boolean, default=True
    Set to `False` to not show the legend.
    Labels on plotted objects are still assigned, so the legend can be created separately by calling `ax.legend()` on the plot axes.

Returns
---
fig : matplotlib Figure object
    Figure object created for the plot.
    Only if `ax=None` is used, else nothing is returned.
ax : list or single object of matplotlib Axes
    Axes created for the different conditions that are plotted.
    Only if `ax=None` is used, else nothing is returned.

Example usage
---
Here we load a dataset with two conditions from the examples directory.

    &gt;&gt;&gt; omics = read_csv(&#34;examples/exampledata3.csv&#34;, sample=&#34;Sample&#34;, time=&#34;Timepoint&#34;, condition=&#34;Condition&#34;)

A direct call to the plot method will create two subplots, one for each of the two conditions in that dataset:

    &gt;&gt;&gt; f, axs = omics.plot(condition=[&#34;a&#34;, &#34;b&#34;])
    &gt;&gt;&gt; f.savefig(&#34;docs/podomics/images/rawdata_plot3.png&#34;)

In this example, the created figure and subplots are returned in the `f` and `axs` variables. 
For that dataset, the plot should look like this:
![image](./images/rawdata_plot3.png)
&#34;&#34;&#34;
        if self.condition is not None:
            if condition is None:
                conditions = self.condition_list
            else:
                conditions = [condition,] if isinstance(condition, str) else condition
                for c in conditions:
                    if c not in self.condition_list:
                        raise ValueError(f&#34;Condition {c} not found in available dataset conditions: {self.condition_list}&#34;)
            df = self.data[self.data[self.condition].isin(conditions)]
        else:
            df = self.data
            conditions = [None,]

        if ax is None:
            fig, axs = pyplot.subplots(1, len(conditions))
            axs[0].set_ylabel(&#34;Feature values&#34;)
            for i in range(len(conditions)):
                axs[i].set_xlabel(&#34;Feature index&#34;)
                if self.condition is not None:
                    axs[i].set_title(&#34;Condition &#34; + conditions[i])
            new_fig = True
        else:
            if len(conditions) &gt; 1:
                raise ValueError(f&#34;Plotting in a given axes is only supported with a single condition, not for the list {conditions}&#34;)
            new_fig = False
            axs = [ax,]
        if isinstance(colormap, str):
            colormap = matplotlib.colormaps[colormap]

        if features is None:
            features = self.features
        else:
            for f in features:
                if f not in self.features:
                    raise ValueError(f&#34;Feature {f} not found in available dataset features: {self.features}&#34;)

        for i, ti in enumerate(self.timepoints):
            df_ti = df[df[self.time] == ti]
            # try to get the color by interpolating numerical values, otherwise just try to interpolate by index
            try:
                cvalue = ti / (self.timepoints[-1] - self.timepoints[0])
            except TypeError:
                cvalue = i / len(self.timepoints) 
            color = colormap(cvalue)
            is_labelled = [False,] * len(axs)
            for j, rowj in enumerate(df_ti.iterrows()):
                if len(conditions) &gt; 1:
                    axi = conditions.index(rowj[1].loc[self.condition])
                else:
                    axi = 0
                ax = axs[axi]
                if not is_labelled[axi]:
                    ax.plot(np.arange(len(features))+(i)*0.3, rowj[1].loc[features].astype(np.float64), &#39;.&#39;, c=color, label=&#34;Time &#34; + str(ti))
                    is_labelled[axi] = True
                else:
                    ax.plot(np.arange(len(features))+(i)*0.3/len(self.timepoints), rowj[1].loc[features].astype(np.float64), &#39;.&#39;, c=color)
        if legend:
            for ax in axs:
                ax.legend()
        if new_fig:
            return fig, axs


from sklearn.utils.validation import (
    check_is_fitted,
    check_random_state,
    _check_sample_weight,
    FLOAT_DTYPES,
)
from sklearn.preprocessing._data import _handle_zeros_in_scale
class MedianScaler(MaxAbsScaler):
    &#34;&#34;&#34;Scale each feature by its median value.

The implementation follows closely the one in [`sklearn.preprocessing.MaxAbsScaler`](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MaxAbsScaler.html).
However, sparse data matrices are not supported for fitting, only for the transformation.

The following documentation is replicated from `sklearn.preprocessing.MaxAbsScaler`. See there for more details and examples.

This estimator scales and translates each feature individually such
that the median value of each feature in the
training set will be 1.0. It does not shift/center the data, and
thus does not destroy any sparsity.

Parameters
----------
copy : bool, default=True
    Set to False to perform inplace scaling and avoid a copy (if the input
    is already a numpy array).
Attributes
----------
scale_ : ndarray of shape (n_features,)
    Per feature relative scaling of the data.
median_ : ndarray of shape (n_features,)
    Per feature median value.
n_features_in_ : int
    Number of features seen during `fit`.
feature_names_in_ : ndarray of shape (`n_features_in_`,)
    Names of features seen during `fit`. Defined only when `X`
    has feature names that are all strings.
n_samples_seen_ : int
    The number of samples processed by the estimator.
&#34;&#34;&#34;
    def fit(self, X, y=None):
        &#34;&#34;&#34;Compute the median value of X for later scaling.

        Parameters
        ----------
        X : array-like of shape (n_samples, n_features)
            The data used to compute the mean and standard deviation
            used for later scaling along the features axis.
        y : None
            Ignored.
        Returns
        -------
        self : object
            Fitted scaler.
        &#34;&#34;&#34;
        self._reset()
        self._validate_params()

        X = self._validate_data(
            X,
            reset=True,
            accept_sparse=None,
            dtype=FLOAT_DTYPES,
            force_all_finite=&#34;allow-nan&#34;,
        )

        self.n_samples_seen_ = X.shape[0]
        self.median_ = np.nanmedian(X, axis=0)
        self.scale_ = _handle_zeros_in_scale(self.median_, copy=True)
        return self

    

    </code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-functions">Functions</h2>
<dl>
<dt id="podomics.dataset.read_csv"><code class="name flex">
<span>def <span class="ident">read_csv</span></span>(<span>file, sample=None, time=None, condition=None, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p>Import dataset from a CSV file.</p>
<p>TODO: info about file format.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>file</code></strong> :&ensp;<code>str, path,</code> or <code>buffer</code></dt>
<dd>The csv file to read from.</dd>
<dt><strong><code>sample</code></strong> :&ensp;<code>str</code>, default=<code>None</code></dt>
<dd>Column header for sample labels. By default, no sample labels are used, and samples can only be indexed by integers.</dd>
<dt><strong><code>time</code></strong> :&ensp;<code>str</code>, default=<code>None</code></dt>
<dd>Column header for time stamps.
If not given, try to find a column header starting with "Time"; raise an error if not found.</dd>
<dt><strong><code>condition</code></strong> :&ensp;<code>str</code>, default=<code>None</code></dt>
<dd>Column header for conditions. By default, no conditions are considered.</dd>
<dt><strong><code>**kwargs</code></strong></dt>
<dd>Are passed to <a href="https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_csv.html">pandas.read_csv()</a> and can be used to adapt to specifics in the file format, e.g.:</dd>
</dl>
<ul>
<li><strong><code>sep</code></strong>: separator string, pandas default is ','</li>
<li><strong><code>comment</code></strong>: character that identifies lines to be interpreted as comments</li>
</ul>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>Dataset</code></strong> :&ensp;<code>object</code></dt>
<dd>The <code><a title="podomics.dataset.Dataset" href="#podomics.dataset.Dataset">Dataset</a></code> object constructed from the data in the given file.</dd>
</dl>
<h2 id="example-usage">Example usage:</h2>
<pre><code>&gt;&gt;&gt; omics = read_csv("examples/exampledata1.csv", sample='Sample')
</code></pre></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def read_csv(file, sample=None, time=None, condition=None, **kwargs):
    &#34;&#34;&#34;
Import dataset from a CSV file.

TODO: info about file format.

Parameters
---
file : str, path, or buffer
    The csv file to read from.
sample : str, default=None
    Column header for sample labels. By default, no sample labels are used, and samples can only be indexed by integers.
time : str, default=None
    Column header for time stamps. 
    If not given, try to find a column header starting with &#34;Time&#34;; raise an error if not found.
condition : str, default=None
    Column header for conditions. By default, no conditions are considered.
**kwargs : 
    Are passed to [pandas.read_csv()](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_csv.html) and can be used to adapt to specifics in the file format, e.g.:
 * **`sep`**: separator string, pandas default is &#39;,&#39;
 * **`comment`**: character that identifies lines to be interpreted as comments

Returns
---
Dataset : object
    The `Dataset` object constructed from the data in the given file.

Example usage:
---
    &gt;&gt;&gt; omics = read_csv(&#34;examples/exampledata1.csv&#34;, sample=&#39;Sample&#39;)
    &#34;&#34;&#34;
    df = pandas.read_csv(file, index_col=sample, **kwargs)
    return Dataset(df, time, condition)</code></pre>
</details>
</dd>
</dl>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="podomics.dataset.Dataset"><code class="flex name class">
<span>class <span class="ident">Dataset</span></span>
<span>(</span><span>data, time=None, condition=None, features=None)</span>
</code></dt>
<dd>
<div class="desc"><p>Data container and various data handling and plotting methods.</p>
<p>The constructor creates a dataset from a pandas Dataframe.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>data</code></strong> :&ensp;<code>pandas DataFrame</code></dt>
<dd>Contains the data to analyse.
It should follow the column layout as explained under <strong><code><a title="podomics.dataset.read_csv" href="#podomics.dataset.read_csv">read_csv()</a></code></strong>.
If sample labels are used, they should be stored in the index of the DataFrame.</dd>
<dt><strong><code>time</code></strong> :&ensp;<code>str</code>, default=<code>None</code></dt>
<dd>Name of the column which contains the timestamps.
If not given, try to find a column header starting with "Time"; raise an error if not found.</dd>
<dt><strong><code>condition</code></strong> :&ensp;<code>str</code>, default=<code>None</code></dt>
<dd>Name of the column which contains the conditions, no conditions are used if this is not given.</dd>
<dt><strong><code>features</code></strong> :&ensp;<code>list</code> of <code>str</code>, default=<code>None</code></dt>
<dd>Column headers to use as features. Use all remaining if not given.</dd>
</dl>
<h2 id="attributes">Attributes</h2>
<dl>
<dt><strong><code>data</code></strong> :&ensp;<code>pandas DataFrame</code></dt>
<dd>Access to the underlying DataFrame</dd>
<dt><strong><code>time</code></strong> :&ensp;<code>str</code></dt>
<dd>Column header used for timestamps</dd>
<dt><strong><code>condition</code></strong> :&ensp;<code>str</code></dt>
<dd>Column header used for condition labels</dd>
<dt><strong><code>features</code></strong> :&ensp;<code>list</code> of <code>str</code></dt>
<dd>Column headers that are referring to features in the dataset</dd>
<dt><strong><code>condition_list</code></strong> :&ensp;<code>list</code> of <code>str</code></dt>
<dd>List of unique condition labels found in the dataset</dd>
<dt><strong><code>timepoints</code></strong> :&ensp;<code>array</code> of <code>numeric</code> or <code>str</code></dt>
<dd>Sorted array of timestamps found in the dataset</dd>
<dt><strong><code>scaling</code></strong> :&ensp;<code>object</code></dt>
<dd>Class used for rescaling the data, see <code><a title="podomics.dataset.Dataset.rescale" href="#podomics.dataset.Dataset.rescale">Dataset.rescale()</a></code>.</dd>
</dl>
<h2 id="example-usage">Example Usage</h2>
<pre><code>&gt;&gt;&gt; df = pandas.read_csv("examples/exampledata1.csv", index_col="Sample")
&gt;&gt;&gt; omics = Dataset(df, time="Timepoint", features=['0', '1', '3', '4', '6'])
&gt;&gt;&gt; omics.time
'Timepoint'
&gt;&gt;&gt; omics.timepoints
array([0.  , 0.25, 0.5 , 0.75, 1.  ])
</code></pre></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class Dataset(object):
    &#34;&#34;&#34;Data container and various data handling and plotting methods.
    &#34;&#34;&#34;
    def __init__(self, data, time=None, condition=None, features=None):
        &#34;&#34;&#34;The constructor creates a dataset from a pandas Dataframe.

Parameters
---
data : pandas DataFrame
    Contains the data to analyse.
    It should follow the column layout as explained under **`read_csv`**.
    If sample labels are used, they should be stored in the index of the DataFrame.
time : str, default=None
    Name of the column which contains the timestamps.
    If not given, try to find a column header starting with &#34;Time&#34;; raise an error if not found.
condition : str, default=None
    Name of the column which contains the conditions, no conditions are used if this is not given.
features : list of str, default=None
    Column headers to use as features. Use all remaining if not given.

Attributes
---
data : pandas DataFrame
    Access to the underlying DataFrame
time : str
    Column header used for timestamps
condition : str
    Column header used for condition labels
features : list of str
    Column headers that are referring to features in the dataset
condition_list : list of str
    List of unique condition labels found in the dataset
timepoints : array of numeric or str
    Sorted array of timestamps found in the dataset
scaling : object
    Class used for rescaling the data, see `Dataset.rescale`.

Example usage
---
    &gt;&gt;&gt; df = pandas.read_csv(&#34;examples/exampledata1.csv&#34;, index_col=&#34;Sample&#34;)
    &gt;&gt;&gt; omics = Dataset(df, time=&#34;Timepoint&#34;, features=[&#39;0&#39;, &#39;1&#39;, &#39;3&#39;, &#39;4&#39;, &#39;6&#39;])
    &gt;&gt;&gt; omics.time
    &#39;Timepoint&#39;
    &gt;&gt;&gt; omics.timepoints
    array([0.  , 0.25, 0.5 , 0.75, 1.  ])
        &#34;&#34;&#34;
        self.data = data
        
        num_cols = len(data.columns)
        if time in data.columns:
            self.time = time
        else:
            if time is None:
                # try to infer time column
                for c in data.columns:
                    if c.startswith(&#34;Time&#34;):
                        time = c
                if time is None:
                    raise ValueError(f&#34;Could not identify a time column in {data.columns[:min(num_cols, 5)]}..., use the `time` parameter to specify one.&#34;)
                else:
                    self.time = time
            else:
                raise ValueError(f&#39;Time column &#34;{time}&#34; not found in data columns: {data.columns[:min(num_cols, 5)]}...&#39;)
        # print(f&#34;Found time column: {self.time}&#34;)

        if condition is None:
            self.condition_list = []
        else:
            if condition in self.data.columns:
                self.condition_list = list(set(data[condition]))
            else:
                raise ValueError(f&#39;Condition column &#34;{condition}&#34; not found in data columns: {data.columns[:min(num_cols, 5)]}...&#39;)
        self.condition = condition
        # print(f&#34;Checked condition column: {self.condition}&#34;)
    
        if features is None:
            self.features = [f for f in data.columns if f not in (self.time, self.condition)]
        else:
            self.features = [f for f in features if (f in data.columns) and f not in (self.time, self.condition)]
            if len(self.features)==0:
                raise ValueError(f&#34;Did not find any of the given feature names in data columns: {self.data.columns[:min(num_cols, 5)]}...&#34;)
        # print(f&#34;Found {len(self.features)} features&#34;)
        # Check that all feature columns are numeric
        # https://stackoverflow.com/questions/19900202/how-to-determine-whether-a-column-variable-is-numeric-or-not-in-pandas-numpy
        from pandas.api.types import is_numeric_dtype
        for f in self.features:
            if not is_numeric_dtype(data[f]):
                raise ValueError(f&#39;Column &#34;{f}&#34; was interpreted as feature, but is not numeric:\n{self.data[f].head()}&#39;)
        # print(f&#34;Verified all feature columns contain numerical data.&#34;)

        self.scaling = None
        timepoints = self.data[self.time].unique()
        timepoints.sort()
        self.timepoints = timepoints

        if self.condition is None:
            columns = [self.time,] + self.features
        else:
            columns = [self.condition, self.time] + self.features
        self.data = data.loc[:, columns]
        # print(f&#34;Transfered data in dataframe of shape {self.data.shape}.&#34;)

        # sort rows first by condition if used, then by timepoint
        if self.condition is not None:
            self.data.sort_values(by=[self.condition, self.time], inplace=True)
        else:
            self.data.sort_values(by=[self.time], inplace=True)
        # print(f&#34;Sorted rows by conditions and timepoints.&#34;)


    def rescale(self, method=MaxAbsScaler(), conditions=None, features=None):
        &#34;&#34;&#34;Rescale the dataset with a transformation method from [sklearn.preprocessing](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.preprocessing).

The data can optionally be filtered with the parameters **`conditions`** and **`features`** before the scaling.

The scaled result is returned as a new **`Dataset`** object.
The applied scaling method is stored in the `scaling` attribute of the returned `Dataset` object.

Parameters
---
method : object, default=MaxAbsScaler() from sklearn.preprocessing
    Instance of a transformation class from [sklearn.preprocessing](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.preprocessing) to be used for resaling the data.
    For median scaling, the class `MedianScaler` provided in this module can be used.
conditions: list of str, default=None
    Restrict the data to conditions given here before scaling.
features: list of str, default=None
    Restrict the data to features given here before scaling.

Returns
---
Dataset : object
    Dataset with the scaled values.
    The `scaling` attribute is set to the object passed in the `method` parameter.

Example usage
---
#### Load a dataset

    &gt;&gt;&gt; omics = read_csv(&#34;examples/exampledata1.csv&#34;, sample=&#34;Sample&#34;)

#### Scaling with maximum absolute value
    &gt;&gt;&gt; scaled = omics.rescale()
    &gt;&gt;&gt; np.allclose([scaled.data[f].max() for f in scaled.features], 1.0)
    True

#### Scaling with median
    &gt;&gt;&gt; scaled = omics.rescale(method=MedianScaler())
    &gt;&gt;&gt; np.allclose([scaled.data[f].median() for f in scaled.features], 1.0)
    True
&#34;&#34;&#34;
        scaler = method
        if features is None:
            features = self.features
        if self.condition is None or conditions is None:
            df = self.data
        else:
            df = self.data[self.data[self.condition].isin(conditions)]
        method.fit(df.loc[:, features])
        df_scaled = df.copy()
        df_scaled.loc[:, features] = method.transform(df.loc[:, features])
        data_scaled = Dataset(df_scaled, time=self.time, condition=self.condition, features=features)
        data_scaled.scaling = method
        return data_scaled

    def plot(self, ax=None, condition=None, features=None, colormap=&#39;viridis&#39;, legend=True):
        &#34;&#34;&#34;Plot the dataset.

This method creates a scatterplot where feature values are plotted vs. feature index.
Data from different timepoints can be colored differently via a colormap.

Parameters
---
ax : matplotlib Axes, default=None
    If given: plot into these axes, otherwise create a new figure to plot in.
condition : str or list of str, default=None
    Choose the condition (from `Dataset.condition_list`) to plot.
    If a list is given, plot all conditions in the list in subplots. This is only possible with a newly created figure.
    Default is to plot all conditions in the dataset.
features : list of str, default=None
    List of features to include in the plot.
    Default is to plot all features in the dataset.
colormap : str or matplotlib Colormap, default=&#39;viridis&#39;
    Colormap to use for choosing the color for each timepoint.
legend : Boolean, default=True
    Set to `False` to not show the legend.
    Labels on plotted objects are still assigned, so the legend can be created separately by calling `ax.legend()` on the plot axes.

Returns
---
fig : matplotlib Figure object
    Figure object created for the plot.
    Only if `ax=None` is used, else nothing is returned.
ax : list or single object of matplotlib Axes
    Axes created for the different conditions that are plotted.
    Only if `ax=None` is used, else nothing is returned.

Example usage
---
Here we load a dataset with two conditions from the examples directory.

    &gt;&gt;&gt; omics = read_csv(&#34;examples/exampledata3.csv&#34;, sample=&#34;Sample&#34;, time=&#34;Timepoint&#34;, condition=&#34;Condition&#34;)

A direct call to the plot method will create two subplots, one for each of the two conditions in that dataset:

    &gt;&gt;&gt; f, axs = omics.plot(condition=[&#34;a&#34;, &#34;b&#34;])
    &gt;&gt;&gt; f.savefig(&#34;docs/podomics/images/rawdata_plot3.png&#34;)

In this example, the created figure and subplots are returned in the `f` and `axs` variables. 
For that dataset, the plot should look like this:
![image](./images/rawdata_plot3.png)
&#34;&#34;&#34;
        if self.condition is not None:
            if condition is None:
                conditions = self.condition_list
            else:
                conditions = [condition,] if isinstance(condition, str) else condition
                for c in conditions:
                    if c not in self.condition_list:
                        raise ValueError(f&#34;Condition {c} not found in available dataset conditions: {self.condition_list}&#34;)
            df = self.data[self.data[self.condition].isin(conditions)]
        else:
            df = self.data
            conditions = [None,]

        if ax is None:
            fig, axs = pyplot.subplots(1, len(conditions))
            axs[0].set_ylabel(&#34;Feature values&#34;)
            for i in range(len(conditions)):
                axs[i].set_xlabel(&#34;Feature index&#34;)
                if self.condition is not None:
                    axs[i].set_title(&#34;Condition &#34; + conditions[i])
            new_fig = True
        else:
            if len(conditions) &gt; 1:
                raise ValueError(f&#34;Plotting in a given axes is only supported with a single condition, not for the list {conditions}&#34;)
            new_fig = False
            axs = [ax,]
        if isinstance(colormap, str):
            colormap = matplotlib.colormaps[colormap]

        if features is None:
            features = self.features
        else:
            for f in features:
                if f not in self.features:
                    raise ValueError(f&#34;Feature {f} not found in available dataset features: {self.features}&#34;)

        for i, ti in enumerate(self.timepoints):
            df_ti = df[df[self.time] == ti]
            # try to get the color by interpolating numerical values, otherwise just try to interpolate by index
            try:
                cvalue = ti / (self.timepoints[-1] - self.timepoints[0])
            except TypeError:
                cvalue = i / len(self.timepoints) 
            color = colormap(cvalue)
            is_labelled = [False,] * len(axs)
            for j, rowj in enumerate(df_ti.iterrows()):
                if len(conditions) &gt; 1:
                    axi = conditions.index(rowj[1].loc[self.condition])
                else:
                    axi = 0
                ax = axs[axi]
                if not is_labelled[axi]:
                    ax.plot(np.arange(len(features))+(i)*0.3, rowj[1].loc[features].astype(np.float64), &#39;.&#39;, c=color, label=&#34;Time &#34; + str(ti))
                    is_labelled[axi] = True
                else:
                    ax.plot(np.arange(len(features))+(i)*0.3/len(self.timepoints), rowj[1].loc[features].astype(np.float64), &#39;.&#39;, c=color)
        if legend:
            for ax in axs:
                ax.legend()
        if new_fig:
            return fig, axs</code></pre>
</details>
<h3>Methods</h3>
<dl>
<dt id="podomics.dataset.Dataset.plot"><code class="name flex">
<span>def <span class="ident">plot</span></span>(<span>self, ax=None, condition=None, features=None, colormap='viridis', legend=True)</span>
</code></dt>
<dd>
<div class="desc"><p>Plot the dataset.</p>
<p>This method creates a scatterplot where feature values are plotted vs. feature index.
Data from different timepoints can be colored differently via a colormap.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>ax</code></strong> :&ensp;<code>matplotlib Axes</code>, default=<code>None</code></dt>
<dd>If given: plot into these axes, otherwise create a new figure to plot in.</dd>
<dt><strong><code>condition</code></strong> :&ensp;<code>str</code> or <code>list</code> of <code>str</code>, default=<code>None</code></dt>
<dd>Choose the condition (from <code>Dataset.condition_list</code>) to plot.
If a list is given, plot all conditions in the list in subplots. This is only possible with a newly created figure.
Default is to plot all conditions in the dataset.</dd>
<dt><strong><code>features</code></strong> :&ensp;<code>list</code> of <code>str</code>, default=<code>None</code></dt>
<dd>List of features to include in the plot.
Default is to plot all features in the dataset.</dd>
<dt><strong><code>colormap</code></strong> :&ensp;<code>str</code> or <code>matplotlib Colormap</code>, default=<code>'viridis'</code></dt>
<dd>Colormap to use for choosing the color for each timepoint.</dd>
<dt><strong><code>legend</code></strong> :&ensp;<code>Boolean</code>, default=<code>True</code></dt>
<dd>Set to <code>False</code> to not show the legend.
Labels on plotted objects are still assigned, so the legend can be created separately by calling <code>ax.legend()</code> on the plot axes.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>fig</code></strong> :&ensp;<code>matplotlib Figure object</code></dt>
<dd>Figure object created for the plot.
Only if <code>ax=None</code> is used, else nothing is returned.</dd>
<dt><strong><code>ax</code></strong> :&ensp;<code>list</code> or <code>single object</code> of <code>matplotlib Axes</code></dt>
<dd>Axes created for the different conditions that are plotted.
Only if <code>ax=None</code> is used, else nothing is returned.</dd>
</dl>
<h2 id="example-usage">Example Usage</h2>
<p>Here we load a dataset with two conditions from the examples directory.</p>
<pre><code>&gt;&gt;&gt; omics = read_csv("examples/exampledata3.csv", sample="Sample", time="Timepoint", condition="Condition")
</code></pre>
<p>A direct call to the plot method will create two subplots, one for each of the two conditions in that dataset:</p>
<pre><code>&gt;&gt;&gt; f, axs = omics.plot(condition=["a", "b"])
&gt;&gt;&gt; f.savefig("docs/podomics/images/rawdata_plot3.png")
</code></pre>
<p>In this example, the created figure and subplots are returned in the <code>f</code> and <code>axs</code> variables.
For that dataset, the plot should look like this:
<img alt="image" src="./images/rawdata_plot3.png"></p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">    def plot(self, ax=None, condition=None, features=None, colormap=&#39;viridis&#39;, legend=True):
        &#34;&#34;&#34;Plot the dataset.

This method creates a scatterplot where feature values are plotted vs. feature index.
Data from different timepoints can be colored differently via a colormap.

Parameters
---
ax : matplotlib Axes, default=None
    If given: plot into these axes, otherwise create a new figure to plot in.
condition : str or list of str, default=None
    Choose the condition (from `Dataset.condition_list`) to plot.
    If a list is given, plot all conditions in the list in subplots. This is only possible with a newly created figure.
    Default is to plot all conditions in the dataset.
features : list of str, default=None
    List of features to include in the plot.
    Default is to plot all features in the dataset.
colormap : str or matplotlib Colormap, default=&#39;viridis&#39;
    Colormap to use for choosing the color for each timepoint.
legend : Boolean, default=True
    Set to `False` to not show the legend.
    Labels on plotted objects are still assigned, so the legend can be created separately by calling `ax.legend()` on the plot axes.

Returns
---
fig : matplotlib Figure object
    Figure object created for the plot.
    Only if `ax=None` is used, else nothing is returned.
ax : list or single object of matplotlib Axes
    Axes created for the different conditions that are plotted.
    Only if `ax=None` is used, else nothing is returned.

Example usage
---
Here we load a dataset with two conditions from the examples directory.

    &gt;&gt;&gt; omics = read_csv(&#34;examples/exampledata3.csv&#34;, sample=&#34;Sample&#34;, time=&#34;Timepoint&#34;, condition=&#34;Condition&#34;)

A direct call to the plot method will create two subplots, one for each of the two conditions in that dataset:

    &gt;&gt;&gt; f, axs = omics.plot(condition=[&#34;a&#34;, &#34;b&#34;])
    &gt;&gt;&gt; f.savefig(&#34;docs/podomics/images/rawdata_plot3.png&#34;)

In this example, the created figure and subplots are returned in the `f` and `axs` variables. 
For that dataset, the plot should look like this:
![image](./images/rawdata_plot3.png)
&#34;&#34;&#34;
        if self.condition is not None:
            if condition is None:
                conditions = self.condition_list
            else:
                conditions = [condition,] if isinstance(condition, str) else condition
                for c in conditions:
                    if c not in self.condition_list:
                        raise ValueError(f&#34;Condition {c} not found in available dataset conditions: {self.condition_list}&#34;)
            df = self.data[self.data[self.condition].isin(conditions)]
        else:
            df = self.data
            conditions = [None,]

        if ax is None:
            fig, axs = pyplot.subplots(1, len(conditions))
            axs[0].set_ylabel(&#34;Feature values&#34;)
            for i in range(len(conditions)):
                axs[i].set_xlabel(&#34;Feature index&#34;)
                if self.condition is not None:
                    axs[i].set_title(&#34;Condition &#34; + conditions[i])
            new_fig = True
        else:
            if len(conditions) &gt; 1:
                raise ValueError(f&#34;Plotting in a given axes is only supported with a single condition, not for the list {conditions}&#34;)
            new_fig = False
            axs = [ax,]
        if isinstance(colormap, str):
            colormap = matplotlib.colormaps[colormap]

        if features is None:
            features = self.features
        else:
            for f in features:
                if f not in self.features:
                    raise ValueError(f&#34;Feature {f} not found in available dataset features: {self.features}&#34;)

        for i, ti in enumerate(self.timepoints):
            df_ti = df[df[self.time] == ti]
            # try to get the color by interpolating numerical values, otherwise just try to interpolate by index
            try:
                cvalue = ti / (self.timepoints[-1] - self.timepoints[0])
            except TypeError:
                cvalue = i / len(self.timepoints) 
            color = colormap(cvalue)
            is_labelled = [False,] * len(axs)
            for j, rowj in enumerate(df_ti.iterrows()):
                if len(conditions) &gt; 1:
                    axi = conditions.index(rowj[1].loc[self.condition])
                else:
                    axi = 0
                ax = axs[axi]
                if not is_labelled[axi]:
                    ax.plot(np.arange(len(features))+(i)*0.3, rowj[1].loc[features].astype(np.float64), &#39;.&#39;, c=color, label=&#34;Time &#34; + str(ti))
                    is_labelled[axi] = True
                else:
                    ax.plot(np.arange(len(features))+(i)*0.3/len(self.timepoints), rowj[1].loc[features].astype(np.float64), &#39;.&#39;, c=color)
        if legend:
            for ax in axs:
                ax.legend()
        if new_fig:
            return fig, axs</code></pre>
</details>
</dd>
<dt id="podomics.dataset.Dataset.rescale"><code class="name flex">
<span>def <span class="ident">rescale</span></span>(<span>self, method=MaxAbsScaler(), conditions=None, features=None)</span>
</code></dt>
<dd>
<div class="desc"><p>Rescale the dataset with a transformation method from <a href="https://scikit-learn.org/stable/modules/classes.html#module-sklearn.preprocessing">sklearn.preprocessing</a>.</p>
<p>The data can optionally be filtered with the parameters <strong><code>conditions</code></strong> and <strong><code>features</code></strong> before the scaling.</p>
<p>The scaled result is returned as a new <strong><code><a title="podomics.dataset.Dataset" href="#podomics.dataset.Dataset">Dataset</a></code></strong> object.
The applied scaling method is stored in the <code>scaling</code> attribute of the returned <code><a title="podomics.dataset.Dataset" href="#podomics.dataset.Dataset">Dataset</a></code> object.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>method</code></strong> :&ensp;<code>object</code>, default=<code>MaxAbsScaler() from sklearn.preprocessing</code></dt>
<dd>Instance of a transformation class from <a href="https://scikit-learn.org/stable/modules/classes.html#module-sklearn.preprocessing">sklearn.preprocessing</a> to be used for resaling the data.
For median scaling, the class <code><a title="podomics.dataset.MedianScaler" href="#podomics.dataset.MedianScaler">MedianScaler</a></code> provided in this module can be used.</dd>
<dt><strong><code>conditions</code></strong> :&ensp;<code>list</code> of <code>str</code>, default=<code>None</code></dt>
<dd>Restrict the data to conditions given here before scaling.</dd>
<dt><strong><code>features</code></strong> :&ensp;<code>list</code> of <code>str</code>, default=<code>None</code></dt>
<dd>Restrict the data to features given here before scaling.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>Dataset</code></strong> :&ensp;<code>object</code></dt>
<dd>Dataset with the scaled values.
The <code>scaling</code> attribute is set to the object passed in the <code>method</code> parameter.</dd>
</dl>
<h2 id="example-usage">Example Usage</h2>
<h4 id="load-a-dataset">Load a dataset</h4>
<pre><code>&gt;&gt;&gt; omics = read_csv("examples/exampledata1.csv", sample="Sample")
</code></pre>
<h4 id="scaling-with-maximum-absolute-value">Scaling with maximum absolute value</h4>
<pre><code>&gt;&gt;&gt; scaled = omics.rescale()
&gt;&gt;&gt; np.allclose([scaled.data[f].max() for f in scaled.features], 1.0)
True
</code></pre>
<h4 id="scaling-with-median">Scaling with median</h4>
<pre><code>&gt;&gt;&gt; scaled = omics.rescale(method=MedianScaler())
&gt;&gt;&gt; np.allclose([scaled.data[f].median() for f in scaled.features], 1.0)
True
</code></pre></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">    def rescale(self, method=MaxAbsScaler(), conditions=None, features=None):
        &#34;&#34;&#34;Rescale the dataset with a transformation method from [sklearn.preprocessing](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.preprocessing).

The data can optionally be filtered with the parameters **`conditions`** and **`features`** before the scaling.

The scaled result is returned as a new **`Dataset`** object.
The applied scaling method is stored in the `scaling` attribute of the returned `Dataset` object.

Parameters
---
method : object, default=MaxAbsScaler() from sklearn.preprocessing
    Instance of a transformation class from [sklearn.preprocessing](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.preprocessing) to be used for resaling the data.
    For median scaling, the class `MedianScaler` provided in this module can be used.
conditions: list of str, default=None
    Restrict the data to conditions given here before scaling.
features: list of str, default=None
    Restrict the data to features given here before scaling.

Returns
---
Dataset : object
    Dataset with the scaled values.
    The `scaling` attribute is set to the object passed in the `method` parameter.

Example usage
---
#### Load a dataset

    &gt;&gt;&gt; omics = read_csv(&#34;examples/exampledata1.csv&#34;, sample=&#34;Sample&#34;)

#### Scaling with maximum absolute value
    &gt;&gt;&gt; scaled = omics.rescale()
    &gt;&gt;&gt; np.allclose([scaled.data[f].max() for f in scaled.features], 1.0)
    True

#### Scaling with median
    &gt;&gt;&gt; scaled = omics.rescale(method=MedianScaler())
    &gt;&gt;&gt; np.allclose([scaled.data[f].median() for f in scaled.features], 1.0)
    True
&#34;&#34;&#34;
        scaler = method
        if features is None:
            features = self.features
        if self.condition is None or conditions is None:
            df = self.data
        else:
            df = self.data[self.data[self.condition].isin(conditions)]
        method.fit(df.loc[:, features])
        df_scaled = df.copy()
        df_scaled.loc[:, features] = method.transform(df.loc[:, features])
        data_scaled = Dataset(df_scaled, time=self.time, condition=self.condition, features=features)
        data_scaled.scaling = method
        return data_scaled</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="podomics.dataset.MedianScaler"><code class="flex name class">
<span>class <span class="ident">MedianScaler</span></span>
<span>(</span><span>*, copy=True)</span>
</code></dt>
<dd>
<div class="desc"><p>Scale each feature by its median value.</p>
<p>The implementation follows closely the one in <a href="https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MaxAbsScaler.html"><code>sklearn.preprocessing.MaxAbsScaler</code></a>.
However, sparse data matrices are not supported for fitting, only for the transformation.</p>
<p>The following documentation is replicated from <code>sklearn.preprocessing.MaxAbsScaler</code>. See there for more details and examples.</p>
<p>This estimator scales and translates each feature individually such
that the median value of each feature in the
training set will be 1.0. It does not shift/center the data, and
thus does not destroy any sparsity.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>copy</code></strong> :&ensp;<code>bool</code>, default=<code>True</code></dt>
<dd>Set to False to perform inplace scaling and avoid a copy (if the input
is already a numpy array).</dd>
</dl>
<h2 id="attributes">Attributes</h2>
<dl>
<dt><strong><code>scale_</code></strong> :&ensp;<code>ndarray</code> of <code>shape (n_features,)</code></dt>
<dd>Per feature relative scaling of the data.</dd>
<dt><strong><code>median_</code></strong> :&ensp;<code>ndarray</code> of <code>shape (n_features,)</code></dt>
<dd>Per feature median value.</dd>
<dt><strong><code>n_features_in_</code></strong> :&ensp;<code>int</code></dt>
<dd>Number of features seen during <code>fit</code>.</dd>
<dt><strong><code>feature_names_in_</code></strong> :&ensp;<code>ndarray</code> of <code>shape (</code>n_features_in_<code>,)</code></dt>
<dd>Names of features seen during <code>fit</code>. Defined only when <code>X</code>
has feature names that are all strings.</dd>
<dt><strong><code>n_samples_seen_</code></strong> :&ensp;<code>int</code></dt>
<dd>The number of samples processed by the estimator.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class MedianScaler(MaxAbsScaler):
    &#34;&#34;&#34;Scale each feature by its median value.

The implementation follows closely the one in [`sklearn.preprocessing.MaxAbsScaler`](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MaxAbsScaler.html).
However, sparse data matrices are not supported for fitting, only for the transformation.

The following documentation is replicated from `sklearn.preprocessing.MaxAbsScaler`. See there for more details and examples.

This estimator scales and translates each feature individually such
that the median value of each feature in the
training set will be 1.0. It does not shift/center the data, and
thus does not destroy any sparsity.

Parameters
----------
copy : bool, default=True
    Set to False to perform inplace scaling and avoid a copy (if the input
    is already a numpy array).
Attributes
----------
scale_ : ndarray of shape (n_features,)
    Per feature relative scaling of the data.
median_ : ndarray of shape (n_features,)
    Per feature median value.
n_features_in_ : int
    Number of features seen during `fit`.
feature_names_in_ : ndarray of shape (`n_features_in_`,)
    Names of features seen during `fit`. Defined only when `X`
    has feature names that are all strings.
n_samples_seen_ : int
    The number of samples processed by the estimator.
&#34;&#34;&#34;
    def fit(self, X, y=None):
        &#34;&#34;&#34;Compute the median value of X for later scaling.

        Parameters
        ----------
        X : array-like of shape (n_samples, n_features)
            The data used to compute the mean and standard deviation
            used for later scaling along the features axis.
        y : None
            Ignored.
        Returns
        -------
        self : object
            Fitted scaler.
        &#34;&#34;&#34;
        self._reset()
        self._validate_params()

        X = self._validate_data(
            X,
            reset=True,
            accept_sparse=None,
            dtype=FLOAT_DTYPES,
            force_all_finite=&#34;allow-nan&#34;,
        )

        self.n_samples_seen_ = X.shape[0]
        self.median_ = np.nanmedian(X, axis=0)
        self.scale_ = _handle_zeros_in_scale(self.median_, copy=True)
        return self</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>sklearn.preprocessing._data.MaxAbsScaler</li>
<li>sklearn.base.OneToOneFeatureMixin</li>
<li>sklearn.base.TransformerMixin</li>
<li>sklearn.utils._set_output._SetOutputMixin</li>
<li>sklearn.base.BaseEstimator</li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="podomics.dataset.MedianScaler.fit"><code class="name flex">
<span>def <span class="ident">fit</span></span>(<span>self, X, y=None)</span>
</code></dt>
<dd>
<div class="desc"><p>Compute the median value of X for later scaling.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>X</code></strong> :&ensp;<code>array-like</code> of <code>shape (n_samples, n_features)</code></dt>
<dd>The data used to compute the mean and standard deviation
used for later scaling along the features axis.</dd>
<dt><strong><code>y</code></strong> :&ensp;<code>None</code></dt>
<dd>Ignored.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>self</code></strong> :&ensp;<code>object</code></dt>
<dd>Fitted scaler.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def fit(self, X, y=None):
    &#34;&#34;&#34;Compute the median value of X for later scaling.

    Parameters
    ----------
    X : array-like of shape (n_samples, n_features)
        The data used to compute the mean and standard deviation
        used for later scaling along the features axis.
    y : None
        Ignored.
    Returns
    -------
    self : object
        Fitted scaler.
    &#34;&#34;&#34;
    self._reset()
    self._validate_params()

    X = self._validate_data(
        X,
        reset=True,
        accept_sparse=None,
        dtype=FLOAT_DTYPES,
        force_all_finite=&#34;allow-nan&#34;,
    )

    self.n_samples_seen_ = X.shape[0]
    self.median_ = np.nanmedian(X, axis=0)
    self.scale_ = _handle_zeros_in_scale(self.median_, copy=True)
    return self</code></pre>
</details>
</dd>
<dt id="podomics.dataset.MedianScaler.fit_transform"><code class="name flex">
<span>def <span class="ident">fit_transform</span></span>(<span>self, X, y=None, **fit_params)</span>
</code></dt>
<dd>
<div class="desc"><p>Fit to data, then transform it.</p>
<p>Fits transformer to <code>X</code> and <code>y</code> with optional parameters <code>fit_params</code>
and returns a transformed version of <code>X</code>.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>X</code></strong> :&ensp;<code>array-like</code> of <code>shape (n_samples, n_features)</code></dt>
<dd>Input samples.</dd>
<dt><strong><code>y</code></strong> :&ensp;<code>array-like</code> of <code>shape (n_samples,)</code> or <code>(n_samples, n_outputs)</code>,
default=<code>None</code></dt>
<dd>Target values (None for unsupervised transformations).</dd>
<dt><strong><code>**fit_params</code></strong> :&ensp;<code>dict</code></dt>
<dd>Additional fit parameters.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>X_new</code></strong> :&ensp;<code>ndarray array</code> of <code>shape (n_samples, n_features_new)</code></dt>
<dd>Transformed array.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def fit_transform(self, X, y=None, **fit_params):
    &#34;&#34;&#34;
    Fit to data, then transform it.

    Fits transformer to `X` and `y` with optional parameters `fit_params`
    and returns a transformed version of `X`.

    Parameters
    ----------
    X : array-like of shape (n_samples, n_features)
        Input samples.

    y :  array-like of shape (n_samples,) or (n_samples, n_outputs), \
            default=None
        Target values (None for unsupervised transformations).

    **fit_params : dict
        Additional fit parameters.

    Returns
    -------
    X_new : ndarray array of shape (n_samples, n_features_new)
        Transformed array.
    &#34;&#34;&#34;
    # non-optimized default implementation; override when a better
    # method is possible for a given clustering algorithm
    if y is None:
        # fit method of arity 1 (unsupervised transformation)
        return self.fit(X, **fit_params).transform(X)
    else:
        # fit method of arity 2 (supervised transformation)
        return self.fit(X, y, **fit_params).transform(X)</code></pre>
</details>
</dd>
<dt id="podomics.dataset.MedianScaler.transform"><code class="name flex">
<span>def <span class="ident">transform</span></span>(<span>self, X)</span>
</code></dt>
<dd>
<div class="desc"><p>Scale the data.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>X</code></strong> :&ensp;<code>{array-like, sparse matrix}</code> of <code>shape (n_samples, n_features)</code></dt>
<dd>The data that should be scaled.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>X_tr</code></strong> :&ensp;<code>{ndarray, sparse matrix}</code> of <code>shape (n_samples, n_features)</code></dt>
<dd>Transformed array.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def transform(self, X):
    &#34;&#34;&#34;Scale the data.

    Parameters
    ----------
    X : {array-like, sparse matrix} of shape (n_samples, n_features)
        The data that should be scaled.

    Returns
    -------
    X_tr : {ndarray, sparse matrix} of shape (n_samples, n_features)
        Transformed array.
    &#34;&#34;&#34;
    check_is_fitted(self)
    X = self._validate_data(
        X,
        accept_sparse=(&#34;csr&#34;, &#34;csc&#34;),
        copy=self.copy,
        reset=False,
        dtype=FLOAT_DTYPES,
        force_all_finite=&#34;allow-nan&#34;,
    )

    if sparse.issparse(X):
        inplace_column_scale(X, 1.0 / self.scale_)
    else:
        X /= self.scale_
    return X</code></pre>
</details>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="podomics" href="index.html">podomics</a></code></li>
</ul>
</li>
<li><h3><a href="#header-functions">Functions</a></h3>
<ul class="">
<li><code><a title="podomics.dataset.read_csv" href="#podomics.dataset.read_csv">read_csv</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="podomics.dataset.Dataset" href="#podomics.dataset.Dataset">Dataset</a></code></h4>
<ul class="">
<li><code><a title="podomics.dataset.Dataset.plot" href="#podomics.dataset.Dataset.plot">plot</a></code></li>
<li><code><a title="podomics.dataset.Dataset.rescale" href="#podomics.dataset.Dataset.rescale">rescale</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="podomics.dataset.MedianScaler" href="#podomics.dataset.MedianScaler">MedianScaler</a></code></h4>
<ul class="">
<li><code><a title="podomics.dataset.MedianScaler.fit" href="#podomics.dataset.MedianScaler.fit">fit</a></code></li>
<li><code><a title="podomics.dataset.MedianScaler.fit_transform" href="#podomics.dataset.MedianScaler.fit_transform">fit_transform</a></code></li>
<li><code><a title="podomics.dataset.MedianScaler.transform" href="#podomics.dataset.MedianScaler.transform">transform</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc"><cite>pdoc</cite> 0.8.1</a>.</p>
</footer>
<script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>
<script>hljs.initHighlightingOnLoad()</script>
</body>
</html>